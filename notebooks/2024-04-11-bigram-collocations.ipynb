{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read corpus index\n",
    "# read docs in corpus one by one\n",
    "# build unigram and bigram histograms\n",
    "# also build sets of UUIDs of docs where words are present\n",
    "\n",
    "# as an end result, we have a mapping: bigram -> freq, set(docs)\n",
    "# and the same mapping for unigrams\n",
    "# based on these, we can select most important bigrams and drop others\n",
    "# also threshold unigrams based on frequency?\n",
    "# we can also filter stopwords in place\n",
    "\n",
    "# do stemming?\n",
    "\n",
    "# new objective:\n",
    "# for each doc, build an index of words (also remove stop-words and too specific ones?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_index = pd.read_json(\"../data/index.json\", lines=True)\n",
    "corpus_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(corpus_dir: pathlib.Path, doc_id: str):\n",
    "    with open(corpus_dir / f\"{doc_id}.txt\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = corpus_index[\"uuid\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_doc_ids = corpus_index.loc[:5, \"uuid\"]\n",
    "head_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DIR = pathlib.Path(\"../data/pages\")\n",
    "\n",
    "doc_contens = tuple(read_doc(CORPUS_DIR, d) for d in head_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_contens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "\n",
    "WORD_PATTERN = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "STOP_WORDS = {\n",
    "    # articles\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    # personals\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"you\",\n",
    "    \"your\",\n",
    "    \"it\",\n",
    "    \"he\",\n",
    "    \"she\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    # proposals\n",
    "    \"to\",\n",
    "    \"of\",\n",
    "    \"in\",\n",
    "    \"at\",\n",
    "    \"by\",\n",
    "    \"from\",\n",
    "    \"out\",\n",
    "    \"on\",\n",
    "    # common verbs\n",
    "    \"be\",\n",
    "    \"is\",\n",
    "    \"was\",\n",
    "    \"were\",\n",
    "    \"have\",\n",
    "    \"been\",\n",
    "    # conjuctions\n",
    "    \"and\",\n",
    "    \"or\",\n",
    "    # other\n",
    "    \"if\",\n",
    "    \"also\",\n",
    "    \"for\",\n",
    "    # modals\n",
    "    \"can\",\n",
    "    \"could\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_collocations(text: str):\n",
    "    text = text.lower()\n",
    "    words = tuple(\n",
    "        filter(\n",
    "            lambda x: x not in STOP_WORDS and x.isalpha(),\n",
    "            map(lambda x: x.group(), WORD_PATTERN.finditer(text)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # build unigram index\n",
    "    unigrams = collections.defaultdict(int)\n",
    "    for w in words:\n",
    "        unigrams[w] += 1\n",
    "\n",
    "    for ug in unigrams:\n",
    "        unigrams[ug] /= len(unigrams)\n",
    "\n",
    "    # build bigram index\n",
    "    bigrams = collections.defaultdict(int)\n",
    "    for word, collocant in itertools.pairwise(words):\n",
    "        bigrams[(word, collocant)] += 1\n",
    "\n",
    "    for bg in bigrams:\n",
    "        bigrams[bg] /= len(bigrams)\n",
    "\n",
    "    def bigram_mi(pair):\n",
    "        x, y = pair\n",
    "        return (bigrams[pair] / unigrams[x] * unigrams[y])\n",
    "\n",
    "    # filter bigram index by mutual information\n",
    "    mi_index = [(p, bigram_mi(p)) for p in bigrams]\n",
    "    mi_index.sort(key=lambda x: x[-1])\n",
    "\n",
    "    cutoff = len(mi_index) // 10\n",
    "    collocations = {k: v for k, v in mi_index[- 5 * cutoff:]}\n",
    "\n",
    "    return unigrams, bigrams, mi_index, collocations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = doc_contens[0]\n",
    "\n",
    "words = tuple(\n",
    "    filter(\n",
    "        lambda x: x not in STOP_WORDS and x.isalpha(),\n",
    "        map(lambda x: x.group(), WORD_PATTERN.finditer(text.lower())),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams, mi_index, bigram_collocations = build_collocations(doc_contens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(unigrams.items()):\n",
    "    print(f\"{k} -> {v:.4f}\")\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "for i, (k, v) in enumerate(bigram_collocations.items()):\n",
    "    print(f\"{k} -> {v:.4f}\")\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_index_values = tuple(v for _, v in mi_index)\n",
    "\n",
    "df = pd.DataFrame({\"mi\": mi_index_values})\n",
    "df[\"freq\"] = df[\"mi\"] > df[\"mi\"].quantile(q=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"mi\", color=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collocations(text: str):\n",
    "    text = text.lower()\n",
    "    words = tuple(\n",
    "        filter(\n",
    "            lambda x: x not in STOP_WORDS,\n",
    "            map(lambda x: x.group(), WORD_PATTERN.finditer(text)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # build unigram index\n",
    "    unigrams = collections.defaultdict(int)\n",
    "    for w in words:\n",
    "        unigrams[w] += 1\n",
    "\n",
    "    # normalize unigrams\n",
    "    for ug in unigrams:\n",
    "        unigrams[ug] /= len(unigrams)\n",
    "\n",
    "    # build bigram index\n",
    "    bigrams = collections.defaultdict(int)\n",
    "    for word, collocant in itertools.pairwise(words):\n",
    "        bigrams[(word, collocant)] += 1\n",
    "\n",
    "    # normalize bigrams\n",
    "    for bg in bigrams:\n",
    "        bigrams[bg] /= len(bigrams)\n",
    "\n",
    "    def bigram_mi(pair):\n",
    "        x, y = pair\n",
    "        return (bigrams[pair] / unigrams[x] * unigrams[y])\n",
    "\n",
    "    # filter bigram index by mutual information\n",
    "    mi_index = [(p, bigram_mi(p)) for p in bigrams]\n",
    "    mi_index.sort(key=lambda x: x[-1])\n",
    "\n",
    "    cutoff = len(mi_index) // 10\n",
    "    bigram_collocations = {k: v for k, v in mi_index[-cutoff:]}\n",
    "\n",
    "    return unigrams, bigram_collocations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Sequence, Tuple\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class DocPlainIndex:\n",
    "    doc_id: str\n",
    "    unigrams: Mapping[str, float]\n",
    "    bigrams: Mapping[Tuple[str, str], float]\n",
    "\n",
    "    def has_word(self, word: str):\n",
    "        return word in self.unigrams\n",
    "    \n",
    "    def has_bigram(self, bigram: Tuple[str, str]):\n",
    "        return bigram in self.bigrams\n",
    "\n",
    "\n",
    "def build_plain_index(corpus_dir: pathlib.Path, doc_ids: Sequence[str]):\n",
    "    def doc_words(doc_id: str):\n",
    "        doc_content = read_doc(corpus_dir, doc_id)\n",
    "        unigrams, bigrams = build_collocations(doc_content)\n",
    "        return DocPlainIndex(doc_id, unigrams=unigrams, bigrams=bigrams)\n",
    "\n",
    "\n",
    "    return tuple(doc_words(d) for d in doc_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_index = build_plain_index(CORPUS_DIR, doc_ids=doc_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_plain_index(idx: Sequence[DocPlainIndex], query: str) -> Sequence[str]:\n",
    "    words = tuple(query.split())\n",
    "\n",
    "    if len(words) > 2:\n",
    "        raise ValueError(\"only single-word and bigram search is supported\")\n",
    "\n",
    "    if len(words) == 2:\n",
    "        return [i for i in idx if i.has_bigram(words)]\n",
    "\n",
    "    (word,) = words\n",
    "    return [i for i in idx if i.has_word(word)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = (\n",
    "    \"game\",\n",
    "    \"chess\",\n",
    "    \"tea\",\n",
    "    \"coffee\",\n",
    "    \"cup tea\",\n",
    "    \"cup coffee\",\n",
    "    \"drink\",\n",
    "    \"drink tea\",\n",
    "    \"drink coffee\",\n",
    "    \"sun\",\n",
    "    \"look\",\n",
    "    \"support\",\n",
    "    \"coins\",\n",
    "    \"play with\",\n",
    "    \"ends with\",\n",
    "    \"destroy\",\n",
    "    \"frog\",\n",
    "    \"fog\",\n",
    "    \"knife\",\n",
    "    \"cut\",\n",
    "    \"watts\",\n",
    "    \"power source\",\n",
    ")\n",
    "\n",
    "print(f\"total documents: {len(plain_index)}\")\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} -> {len(search_plain_index(plain_index, word))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
